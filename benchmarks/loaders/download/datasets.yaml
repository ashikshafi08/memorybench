# Dataset Configuration for CodeChunkBench
# All URLs, repos, and task types are defined here.
# Edit this file to add new datasets or task types.

datasets:
  repoeval:
    description: "RepoEval code completion benchmark"
    envVar: REPOEVAL_DATA_DIR

    # Static dataset files (JSONL tasks)
    sources:
      datasets:
        type: zip
        url: https://github.com/microsoft/CodeT/raw/main/RepoCoder/datasets/datasets.zip
        extractTo: datasets

    # Task types with their respective repository sources
    taskTypes:
      function:
        description: "Function-level code completion"
        repos:
          type: zip
          url: https://github.com/Veronicium/repoeval_debug/raw/main/function_level.zip
          extractTo: repositories
        # Curated repos for function-level (original 6 from paper + 2 additional repos in dataset)
        repoList:
          - CarperAI_trlx
          - amazon-science_patchcore-inspection
          - deepmind_tracr
          - facebookresearch_omnivore
          - google_lightweight_mmm
          - leopard-ai_betty
          - lucidrains_imagen-pytorch
          - maxhumber_redframes

      line:
        description: "Line-level code completion"
        repos:
          # Line-level repos need to be cloned from GitHub (no pre-packaged zip)
          type: zip
          url: https://github.com/Veronicium/repoeval_debug/raw/main/line_level.zip
          extractTo: repositories/line
        # Actual repos used in line-level tasks (from JSONL file):
        repoList:
          - huggingface_diffusers
          - pytorch_rl
          - nerfstudio-project_nerfstudio
          - alibaba_FederatedScope
          - awslabs_fortuna
          - google_vizier
          - huggingface_evaluate
          - opendilab_ACE

      api:
        description: "API-level code completion"
        repos:
          type: zip
          url: https://github.com/Veronicium/repoeval_debug/raw/main/api_level.zip
          extractTo: repositories/api
        # Repos used in API-level tasks
        repoList:
          - microsoft_DeepSpeed
          - facebookresearch_hydra
          - google-research_text-to-text-transfer-transformer
          - facebookresearch_ParlAI
          - Lightning-AI_lightning

  repobench-r:
    description: "RepoBench cross-file retrieval benchmark"
    envVar: REPOBENCH_DATA_DIR

    languages:
      python:
        source:
          type: huggingface_parquet
          dataset: tianyang/repobench_python_v1.1
          config: default
          split: cross_file_first
        outputFile: python.jsonl

      java:
        source:
          type: huggingface_parquet
          dataset: tianyang/repobench_java_v1.1
          config: default
          split: cross_file_first
        outputFile: java.jsonl

  crosscodeeval:
    description: "CrossCodeEval multi-language code completion"
    envVar: CROSSCODEEVAL_DATA_DIR

    source:
      type: tar_xz
      url: https://raw.githubusercontent.com/amazon-science/cceval/main/data/crosscodeeval_data.tar.xz

    languages:
      - python
      - java
      - typescript
      - csharp

    # After extraction, copy from {lang}/line_completion_oracle_bm25.jsonl to {lang}.jsonl
    postExtract:
      pattern: "{lang}/line_completion_oracle_bm25.jsonl"
      outputPattern: "{lang}.jsonl"

  swebench-lite:
    description: "SWE-bench Lite software engineering benchmark"
    envVar: SWEBENCH_DATA_DIR

    source:
      type: huggingface_json
      dataset: princeton-nlp/SWE-bench_Lite
      config: default
      split: test
      maxRows: 10000
    outputFile: swebench-lite.json
